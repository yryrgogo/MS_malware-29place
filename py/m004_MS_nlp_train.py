is_debug = 0
is_multi = 0
import os
import re
import sys
import glob
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
HOME = os.path.expanduser('~')
sys.path.append(f"{HOME}/kaggle/data_analysis/library/")
sys.path.append(f"../py/")
import MS_utils
import utils, ml_utils, kaggle_utils
from utils import logger_func
try:
    if not logger:
        logger=logger_func()
except NameError:
    logger=logger_func()
import time
from sklearn.metrics import roc_auc_score, mean_squared_error

# Columns
key, target, ignore_list = MS_utils.get_basic_var()

if is_multi:
    import tensorflow as tf
    from keras.utils.training_utils import multi_gpu_model # add
    gpu_count = 4 # add

try:
    comment = sys.argv[1]
except IndexError:
    comment = "-"
start_time = time.time()

#========================================================================
# Keras
from os.path import dirname
#sys.path.append(dirname(dirname(__file__)))

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D
from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate
from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D
from keras.optimizers import Adam, SGD
from keras.models import Model
from keras import backend as K
from keras.engine.topology import Layer
from keras import initializers, regularizers, constraints, optimizers, layers
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

from keras import initializers
from keras.engine import InputSpec, Layer
from keras import backend as K

# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py
class AttentionWeightedAverage(Layer):
    """
    Computes a weighted average of the different channels across timesteps.
    Uses 1 parameter pr. channel to compute the attention value for a single timestep.
    """

    def __init__(self, return_attention=False, **kwargs):
        self.init = initializers.get('uniform')
        self.supports_masking = True
        self.return_attention = return_attention
        super(AttentionWeightedAverage, self).__init__(** kwargs)

    def build(self, input_shape):
        self.input_spec = [InputSpec(ndim=3)]
        assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[2], 1),
                                 name='{}_W'.format(self.name),
                                 initializer=self.init)
        self.trainable_weights = [self.W]
        super(AttentionWeightedAverage, self).build(input_shape)

    def call(self, x, mask=None):
        # computes a probability distribution over the timesteps
        # uses 'max trick' for numerical stability
        # reshape is done to avoid issue with Tensorflow
        # and 1-dimensional weights
        logits = K.dot(x, self.W)
        x_shape = K.shape(x)
        logits = K.reshape(logits, (x_shape[0], x_shape[1]))
        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))

        # masked timesteps have zero weight
        if mask is not None:
            mask = K.cast(mask, K.floatx())
            ai = ai * mask
        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())
        weighted_input = x * K.expand_dims(att_weights)
        result = K.sum(weighted_input, axis=1)
        if self.return_attention:
            return [result, att_weights]
        return result

    def get_output_shape_for(self, input_shape):
        return self.compute_output_shape(input_shape)

    def compute_output_shape(self, input_shape):
        output_len = input_shape[2]
        if self.return_attention:
            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]
        return (input_shape[0], output_len)

    def compute_mask(self, input, input_mask=None):
        if isinstance(input_mask, list):
            return [None] * len(input_mask)
        else:
            return None


def build_model(nb_words, max_length, embedding_size=200):
    #  with tf.device("/cpu:0"): # add
    inp = Input(shape=(max_length,))
#     x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)
    x = Embedding(nb_words, embedding_size, input_length=max_length)(inp)
    x = SpatialDropout1D(0.3)(x)
    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)
    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)
    max_pool1 = GlobalMaxPooling1D()(x1)
    max_pool2 = GlobalMaxPooling1D()(x2)
    conc = Concatenate()([max_pool1, max_pool2])
    predictions = Dense(1, activation='sigmoid')(conc)
    model = Model(inputs=inp, outputs=predictions)
    adam = optimizers.SGD(lr=learning_rate)

    if is_multi:
        model = multi_gpu_model(model, gpus=gpu_count)
    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])
    return model
#========================================================================


#========================================================================
# Load Feature Matrix
base = utils.read_df_pkl('../input/base_Av*')
tx_train = utils.read_df_pkl('../input/0306_MS_NLP_train*').values
x_test = utils.read_df_pkl('../input/0306_MS_NLP_test*').values
#========================================================================

#========================================================================
# Make Validation
seed = 1208
fold_n = 4
vi_col = 'f000_AvSigVersion'
base_train = base[~base[target].isnull()]
base_test = base[base[target].isnull()]
# sort前にyを取得
y = base_train[target].values
base_train.sort_values(vi_col, inplace=True)
base = base[[key, target]]

if is_debug:
    tx_train = tx_train[:10000, :]
    x_test = x_test[:1000, :]
    base_train = base_train[[key, target]].head(10000)
    base_test = base_test[[key, target]].head(1000)
else:
    base_train = base_train[[key, target]]
    base_test = base_test[[key, target]]

from sklearn.model_selection import KFold
# define the validation scheme
cv = KFold(n_splits=fold_n, shuffle=False, random_state=seed)
kfold = list(cv.split(base_train, base_train[target]))
# kfold = MS_utils.get_kfold(base=base)
# kfold = zip(*kfold)

#========================================================================

#========================================================================
# hyperparameters
embedding_size = 200
learning_rate = 0.001
batch_size = 2**9
num_epoch = 8
if is_debug:
    batch_size = 2**4
    num_epoch = 2
if is_multi:
    batch_size *= gpu_count

max_length = tx_train.shape[1]
nb_words = max_length
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=0),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4)
]
#========================================================================

#========================================================================
# Result Box
is_oof = 1
result_list = []
score_list = []
oof_pred = np.zeros(len(tx_train))
test_pred = np.zeros(len(x_test))
#========================================================================


#========================================================================
# Train & Prediction Start
for fold_no, (trn_idx, val_idx) in enumerate(kfold):

    with utils.timer(f'Fold{fold_no} Train'):
        #========================================================================
        # Make Dataset
        X_train, y_train = tx_train[trn_idx, :], y[trn_idx]
        X_val, y_val = tx_train[val_idx, :], y[val_idx]

        print(X_train.shape, X_val.shape)
        print(f"Target Min --- Train: {y_train.min()} Valid: {y_val.min()}")
        print(f"Target Min Count --- Train: {np.sum(y_train==y_train.min())} Valid: {np.sum(y_val==y_val.min())}")

        model = build_model(max_length=max_length, nb_words=nb_words, embedding_size=embedding_size)
        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epoch-1, verbose=2, callbacks=callbacks)
        test_pred += 0.3*np.squeeze(model.predict(x_test, batch_size=batch_size, verbose=2))

        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=1, verbose=2)
        test_pred += 0.7*np.squeeze(model.predict(x_test, batch_size=batch_size, verbose=2))

        print(f"Test Prob Shape: {test_pred.shape}")

        #========================================================================
        # OOF
        y_pred = np.squeeze(model.predict(X_val, batch_size=batch_size, verbose=2))
        oof_pred[val_idx] = y_pred
        #========================================================================

        del model
        gc.collect()
        K.clear_session()

        #========================================================================
        # Scoring
        score = roc_auc_score(y_val, y_pred)
        print(f'--- AUC: {score} ---')
        score_list.append(score)
        #========================================================================

        #========================================================================
        # Enroute Saving
        #  base_test['prediction'] = (test_pred/fold_no+1)
        #  save_path = f'../stack/{start_time}_NN_NLP_Enroute{comment}_feat{X_train.shape[1]}_FOLD{score}_LB'
        #  utils.to_pkl_gzip(obj=base_test[[key, 'prediction']], path=save_path)

        #========================================================================

cv_score = np.mean(score_list)
logger.info(f'''
#========================================================================
# CV SCORE AVG: {cv_score}
#========================================================================''')

test_pred /= fold_no+1

base_train['prediction'] = oof_pred
base_test['prediction'] = test_pred

#========================================================================
# Stacking
if is_oof:
    df_stack = pd.concat([base_train, base_test], axis=0, ignore_index=True)
    print(f"DF Stack Shape: {df_stack.shape}")
#========================================================================

if is_debug:
    sys.exit()

utils.to_pkl_gzip(obj=df_stack, path=f'../stack/{start_time}_NN_NLP_{comment}_feat{X_train.shape[1]}_fold{fold_n}_CV{cv_score}_LB')
