is_make = 1
is_save = 1
is_debug = 0
is_fe = 0
is_multi = 0
import os
import re
import sys
import glob
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
HOME = os.path.expanduser('~')
sys.path.append(f"{HOME}/kaggle/data_analysis/library/")
sys.path.append(f"../py/")
import MS_utils
import utils, ml_utils, kaggle_utils
from utils import logger_func
try:
    if not logger:
        logger=logger_func()
except NameError:
    logger=logger_func()
import time
from sklearn.metrics import roc_auc_score, mean_squared_error

# Columns
key, target, ignore_list = MS_utils.get_basic_var()


if is_multi:
    import tensorflow as tf
    from keras.utils.training_utils import multi_gpu_model # add
    gpu_count = 4 # add


#========================================================================
# Keras
from os.path import dirname
#sys.path.append(dirname(dirname(__file__)))

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D
from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate
from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D
from keras.optimizers import Adam, SGD
from keras.models import Model
from keras import backend as K
from keras.engine.topology import Layer
from keras import initializers, regularizers, constraints, optimizers, layers
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

from keras import initializers
from keras.engine import InputSpec, Layer
from keras import backend as K


base = utils.read_df_pkl(path='../input/base_Av*')
if is_make:
    #========================================================================
    # Dataset Load
    with utils.timer('Download Train and Test Data.\n'):
        train, test = MS_utils.get_dataset(base=base, feat_path='../features/4_winner/*.gz', is_cat_encode=False)


        nlp_cols = [
            'Engine'
            ,'OSVersion'
            ,'AppVersion'
            ,'AvSigVersion'
            ,'SkuEdition'
            ,'SmartScreen'
            ,'Census_OSArchitecture'
            ,'AVProductStatesIdentifier'
            ,'AVProductsInstalled'
            ,'CountryIdentifier'
            ,'CityIdentifier'
            ,'OrganizationIdentifier'
            ,'GeoNameIdentifier'
            ,'LocaleEnglishNameIdentifier'
            ,'OsBuild'
            ,'OsBuildLab'
            ,'Census_OEMNameIdentifier'
            ,'Census_OEMModelIdentifier'
            ,'Census_ProcessorCoreCount'
            ,'Census_ProcessorManufacturerIdentifier'
            ,'Census_PrimaryDiskTypeName'
            ,'Census_OSBranch'
            ,'Census_OSBuildNumber'
            ,'Census_OSBuildRevision'
            ,'Census_OSEdition'
            ,'Census_OSInstallTypeName'
            ,'Census_OSInstallLanguageIdentifier'
            ,'Census_OSUILocaleIdentifier'
            ,'Census_OSWUAutoUpdateOptionsName'
            ,'Census_GenuineStateName'
            ,'Census_ActivationChannel'
            ,'Census_FlightRing'
            ,'Census_FirmwareManufacturerIdentifier'
            ,'Census_FirmwareVersionIdentifier'
        ]

        num_list = [
            'Census_PrimaryDiskTotalCapacity'
            ,'Census_SystemVolumeTotalCapacity'
            ,'Census_TotalPhysicalRAM'
            ,'Census_InternalPrimaryDiagonalDisplaySizeInInches'
            ,'Census_InternalPrimaryDisplayResolutionHorizontal'
            ,'Census_InternalPrimaryDisplayResolutionVertical'
            ,'Census_InternalBatteryNumberOfCharges'
            ,'Wdft_IsGamer'
        ]

        tmp_list = []
        for col in train.columns:
            for n_col in nlp_cols:
                if col.count(n_col):
                    tmp_list.append(col)
        nlp_cols = list(set(tmp_list))

        tmp_list = []
        for col in train.columns:
            for n_col in num_list:
                if col.count(n_col):
                    tmp_list.append(col)
        num_list = list(set(tmp_list))

        # Text
        nlp_train = train[[key, target] + nlp_cols]
        nlp_test = test[[key] + nlp_cols]

        # Numeric
        num_train = train[num_list]
        num_test = test[num_list]

        if is_debug:
            nlp_train = nlp_train.head(10000)
            nlp_test = nlp_test.head(1000)
            num_train = num_train.head(10000)
            num_test = num_test.head(1000)

        nlp_train = pd.concat([nlp_train, nlp_test], axis=0)
        nlp_train.drop("f000_AvSigVersion", axis=1, inplace=True)
        print("Data Load Complete!!")
        
        del train, test
        gc.collect()

#========================================================================


#========================================================================
# FE


if is_fe:
    #========================================================================
    # Engineのversionを分ける
    #========================================================================

    en_col = 'EngineVersion'
    engine = nlp_train[en_col]
    engine = engine.map(lambda x: x[4:])
    df_en = pd.DataFrame([en.split('.') for en in engine.values], columns=['Engine-Major', 'Engine-Sub'])
    col = 'Engine-Major'
    feature = df_en[col].values.astype('float32')
    nlp_train[col] = feature
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')

    col = 'Engine-Sub'
    feature = df_en[col].values.astype('float32')
    nlp_train[col] = feature

    del nlp_train[en_col]
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')

    #========================================================================
    # 'AvSigVersion'のversionを分ける
    #========================================================================

    vi_col = 'AvSigVersion'
    vi = nlp_train[vi_col]
    vi_pre = vi.map(lambda x: np.int(x[:4].replace('.', '').replace('&', '')))
    vi_suf = vi.map(lambda x: np.int(x[5:].replace('.', '').replace("x17;311440", '311440')))
    
    df_vi = pd.concat([vi_pre, vi_suf], axis=1)
    df_vi.columns = ['AvSigVersion-Major', 'AvSigVersion-Sub']
    
    col = 'AvSigVersion-Major'
    feature = df_vi[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'AvSigVersion-Sub'
    feature = df_vi[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    
    nlp_train[col] = feature
    
    del nlp_train[vi_col]
    
    #========================================================================
    # OSのversionを分ける
    #========================================================================
    
    os_col = 'Census_OSVersion'
    os = nlp_train[os_col]
    os = os.map(lambda x: x.replace('10.0.', ''))
    
    os_list = [v.split('.') for v in os.values]
    
    if is_debug:
        df_os = pd.DataFrame(os_list, columns=["OSVersion-Major", "OSVersion-Sub"])
    else:
        df_os = pd.DataFrame(os_list, columns=["OSVersion-Major", "OSVersion-Sub", '0', '1'])
    
    col = 'OSVersion-Major'
    feature = df_os[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'OSVersion-Sub'
    feature = df_os[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    del nlp_train[os_col]
    
    #========================================================================
    # Appのversionを分ける
    #========================================================================
    
    app_col = 'AppVersion'
    app = nlp_train[app_col]
    app = app.map(lambda x: x[2:])
    
    app_list = [v.split('.') for v in app.values]
    df_app = pd.DataFrame(app_list, columns=["AppVersion-1", "AppVersion-2", "AppVersion-3"])
    
    col = 'AppVersion-1'
    feature = df_app[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'AppVersion-2'
    feature = df_app[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'AppVersion-3'
    feature = df_app[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    del nlp_train[app_col]
    print("Feature Engineering Complete!!")
    #========================================================================
    
    
#========================================================================
# Cleansing
with utils.timer("Transforming..."):
    if is_make:

        org_cols = [col  if col in ignore_list else str(num) for num, col in enumerate(nlp_train.columns)]
        new_cols = [col  if col in ignore_list else str(num) for num, col in enumerate(nlp_train.columns)]
        nlp_train.columns = new_cols

        nlp_dtype_dict = nlp_train.dtypes

        for col in tqdm(nlp_train.columns):
            if col in ignore_list:
                continue

            mode = nlp_train[col].mode().values[0]

            nlp_train[col].fillna(mode, inplace=True)

            if str(nlp_dtype_dict[col]).count('int') or str(nlp_dtype_dict[col]).count('float'):
                nlp_train[col] = nlp_train[col].map(lambda x: col + '_' + str(int(x)))

#========================================================================

#========================================================================
# Tokenize
with utils.timer("Transforming..."):
    if is_make:
        ## Tokenize the sentences
        use_cols = [col for col in nlp_train.columns if col not in ignore_list]
        tx_col = "text"
        # nlp_train[tx_col] = nlp_train[use_cols].apply(lambda x: ' '.join([ str(tx) for tx in x]), axis=1)
        nlp_train[tx_col] = nlp_train[use_cols].apply(lambda x: ' '.join(x.values.tolist()), axis=1)
        #  if is_save:
        #      utils.to_df_pkl(df=nlp_train[[key, tx_col, target]], path='../input/', fname=f'0305_MS_NLP_feat{len(use_cols)}')
    else:
        #  len_train = base[~base[target].isnull()]
        nlp_train = utils.read_df_pkl(path='../input/0305_MS_NLP_feat*')

text_list = nlp_train[tx_col].values.tolist()

print(nlp_train.shape)

max_features = 10000
nb_words = max_features
max_length = 100
tokenizer = Tokenizer(num_words=max_features, split=" ")
tokenizer.fit_on_texts(text_list)
del text_list
gc.collect()
#========================================================================

#========================================================================
# Make Feature Matrix
with utils.timer('Text to Sequences...'):
    nlp_test = nlp_train[nlp_train[target].isnull()]
    nlp_train = nlp_train[~nlp_train[target].isnull()]
    y = nlp_train[target].values
    tx_train = nlp_train[tx_col].copy()
    tx_test = nlp_test[tx_col].copy()
    del nlp_train, nlp_test
    gc.collect()

    x_train = tokenizer.texts_to_sequences(tx_train)
    x_test = tokenizer.texts_to_sequences(tx_test)

del tx_train, tx_test
gc.collect()

# Pad the sentences 
with utils.timer('Pad Sequences...'):
    print("Pad Sequences Start!!")
    train_word_sequences = pad_sequences(x_train, maxlen=max_length, padding='post')
    test_word_sequences = pad_sequences(x_test, maxlen=max_length, padding='post')
    # train_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')
    # test_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')

    pred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)
del x_train, x_test
gc.collect()

#========================================================================
# Numericの結合
is_num = 1
if is_num:
    train_word_sequences = np.hstack((train_word_sequences, num_train.values))
    test_word_sequences = np.hstack((test_word_sequences, num_test.values))

print(f"Train: {train_word_sequences.shape} | Test: {test_word_sequences.shape}")
print(train_word_sequences[:1])
print(test_word_sequences[:1])

# Train Test Set
tx_train = train_word_sequences.copy()
x_test = test_word_sequences.copy()
if is_save:
    utils.to_df_pkl(df=pd.DataFrame(tx_train), path='../input', fname=f'0306_MS_NLP_train_only_feat{tx_train.shape[1]}')
    utils.to_df_pkl(df=pd.DataFrame(x_test), path='../input', fname=f'0306_MS_NLP_test_only_feat{x_test.shape[1]}')
del train_word_sequences, test_word_sequences
gc.collect()
#========================================================================
