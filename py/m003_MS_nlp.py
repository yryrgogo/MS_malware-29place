is_make = 0
is_debug = 0
is_fe = 0
is_multi = 0
import os
import re
import sys
import glob
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
HOME = os.path.expanduser('~')
sys.path.append(f"{HOME}/kaggle/data_analysis/library/")
sys.path.append(f"../py/")
import MS_utils
import utils, ml_utils, kaggle_utils
from utils import logger_func
try:
    if not logger:
        logger=logger_func()
except NameError:
    logger=logger_func()
import time
from sklearn.metrics import roc_auc_score, mean_squared_error

# Columns
key, target, ignore_list = MS_utils.get_basic_var()


if is_multi:
    import tensorflow as tf
    from keras.utils.training_utils import multi_gpu_model # add
    gpu_count = 4 # add


#========================================================================
# Keras
from os.path import dirname
#sys.path.append(dirname(dirname(__file__)))

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D
from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D
from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate
from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D
from keras.optimizers import Adam, SGD
from keras.models import Model
from keras import backend as K
from keras.engine.topology import Layer
from keras import initializers, regularizers, constraints, optimizers, layers
from keras import callbacks
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

from keras import initializers
from keras.engine import InputSpec, Layer
from keras import backend as K

from adabound import AdaBound

# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py
class AttentionWeightedAverage(Layer):
    """
    Computes a weighted average of the different channels across timesteps.
    Uses 1 parameter pr. channel to compute the attention value for a single timestep.
    """

    def __init__(self, return_attention=False, **kwargs):
        self.init = initializers.get('uniform')
        self.supports_masking = True
        self.return_attention = return_attention
        super(AttentionWeightedAverage, self).__init__(** kwargs)

    def build(self, input_shape):
        self.input_spec = [InputSpec(ndim=3)]
        assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[2], 1),
                                 name='{}_W'.format(self.name),
                                 initializer=self.init)
        self.trainable_weights = [self.W]
        super(AttentionWeightedAverage, self).build(input_shape)

    def call(self, x, mask=None):
        # computes a probability distribution over the timesteps
        # uses 'max trick' for numerical stability
        # reshape is done to avoid issue with Tensorflow
        # and 1-dimensional weights
        logits = K.dot(x, self.W)
        x_shape = K.shape(x)
        logits = K.reshape(logits, (x_shape[0], x_shape[1]))
        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))

        # masked timesteps have zero weight
        if mask is not None:
            mask = K.cast(mask, K.floatx())
            ai = ai * mask
        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())
        weighted_input = x * K.expand_dims(att_weights)
        result = K.sum(weighted_input, axis=1)
        if self.return_attention:
            return [result, att_weights]
        return result

    def get_output_shape_for(self, input_shape):
        return self.compute_output_shape(input_shape)

    def compute_output_shape(self, input_shape):
        output_len = input_shape[2]
        if self.return_attention:
            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]
        return (input_shape[0], output_len)

    def compute_mask(self, input, input_mask=None):
        if isinstance(input_mask, list):
            return [None] * len(input_mask)
        else:
            return None


def build_model(nb_words, max_length, embedding_size=200):
    #  with tf.device("/cpu:0"): # add
    inp = Input(shape=(max_length,))
#     x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)
    x = Embedding(nb_words, embedding_size, input_length=max_length)(inp)
    x = SpatialDropout1D(0.3)(x)
    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)
    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)
    max_pool1 = GlobalMaxPooling1D()(x1)
    max_pool2 = GlobalMaxPooling1D()(x2)
    conc = Concatenate()([max_pool1, max_pool2])
    predictions = Dense(1, activation='sigmoid')(conc)
    model = Model(inputs=inp, outputs=predictions)
    #  adam = optimizers.SGD(lr=learning_rate)

    optm = AdaBound(lr=learning_rate,
                final_lr=0.1,
                gamma=1e-03,
                weight_decay=0.,
                amsbound=False)

    if is_multi:
        model = multi_gpu_model(model, gpus=gpu_count)
    model.compile(optimizer=optm, loss='binary_crossentropy', metrics=['accuracy'])
    return model
#========================================================================


if is_make:
    #========================================================================
    # Dataset Load
    print('Download Train and Test Data.\n')
    train, test = MS_utils.get_dataset(feat_path='../features/4_winner/*.gz')


    nlp_cols = [
        'SkuEdition'
        ,'SmartScreen'
        ,'EngineVersion'
        ,'AppVersion'
        ,'AvSigVersion'
        ,'Census_OSArchitecture'
        ,'AVProductStatesIdentifier'
        ,'AVProductsInstalled'
        ,'CountryIdentifier'
        ,'CityIdentifier'
        ,'OrganizationIdentifier'
        ,'GeoNameIdentifier'
        ,'LocaleEnglishNameIdentifier'
        ,'OsBuild'
        ,'OsBuildLab'
        ,'Census_OSVersion'
        ,'Census_OEMNameIdentifier'
        ,'Census_OEMModelIdentifier'
        ,'Census_ProcessorCoreCount'
        ,'Census_ProcessorManufacturerIdentifier'
        ,'Census_PrimaryDiskTypeName'
        ,'Census_OSBranch'
        ,'Census_OSBuildNumber'
        ,'Census_OSBuildRevision'
        ,'Census_OSEdition'
        ,'Census_OSInstallTypeName'
        ,'Census_OSInstallLanguageIdentifier'
        ,'Census_OSUILocaleIdentifier'
        ,'Census_OSWUAutoUpdateOptionsName'
        ,'Census_GenuineStateName'
        ,'Census_ActivationChannel'
        ,'Census_FlightRing'
        ,'Census_FirmwareManufacturerIdentifier'
        ,'Census_FirmwareVersionIdentifier'
    ]
    
    num_list = [
        'Census_PrimaryDiskTotalCapacity'
        ,'Census_SystemVolumeTotalCapacity'
        ,'Census_TotalPhysicalRAM'
        ,'Census_InternalPrimaryDiagonalDisplaySizeInInches'
        ,'Census_InternalPrimaryDisplayResolutionHorizontal'
        ,'Census_InternalPrimaryDisplayResolutionVertical'
        ,'Census_InternalBatteryNumberOfCharges'
        ,'Wdft_IsGamer'
    ]
    
    tmp_list = []
    for col in train.columns:
        for n_col in nlp_cols:
            if col.count(n_col):
                tmp_list.append(col)
    nlp_cols = list(set(tmp_list))

    tmp_list = []
    for col in train.columns:
        for n_col in num_list:
            if col.count(n_col):
                tmp_list.append(col)
    num_list = list(set(tmp_list))

    # Text
    nlp_train = train[[key, target] + nlp_cols]
    nlp_test = test[[key] + nlp_cols]

    # Numeric
    num_train = train[num_list]
    num_test = test[num_list]

if is_debug:
    nlp_train = nlp_train.head(10000)
    nlp_test = nlp_test.head(1000)
    num_train = num_train.head(10000)
    num_test = num_test.head(1000)

nlp_train = pd.concat([nlp_train, nlp_test], axis=0)
print("Data Load Complete!!")

#========================================================================


#========================================================================
# FE


if is_fe:
    #========================================================================
    # Engineのversionを分ける
    #========================================================================

    en_col = 'EngineVersion'
    engine = nlp_train[en_col]
    engine = engine.map(lambda x: x[4:])
    df_en = pd.DataFrame([en.split('.') for en in engine.values], columns=['Engine-Major', 'Engine-Sub'])
    col = 'Engine-Major'
    feature = df_en[col].values.astype('float32')
    nlp_train[col] = feature
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')

    col = 'Engine-Sub'
    feature = df_en[col].values.astype('float32')
    nlp_train[col] = feature

    del nlp_train[en_col]
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')

    #========================================================================
    # 'AvSigVersion'のversionを分ける
    #========================================================================

    vi_col = 'AvSigVersion'
    vi = nlp_train[vi_col]
    vi_pre = vi.map(lambda x: np.int(x[:4].replace('.', '').replace('&', '')))
    vi_suf = vi.map(lambda x: np.int(x[5:].replace('.', '').replace("x17;311440", '311440')))
    
    df_vi = pd.concat([vi_pre, vi_suf], axis=1)
    df_vi.columns = ['AvSigVersion-Major', 'AvSigVersion-Sub']
    
    col = 'AvSigVersion-Major'
    feature = df_vi[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'AvSigVersion-Sub'
    feature = df_vi[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    
    nlp_train[col] = feature
    
    del nlp_train[vi_col]
    
    #========================================================================
    # OSのversionを分ける
    #========================================================================
    
    os_col = 'Census_OSVersion'
    os = nlp_train[os_col]
    os = os.map(lambda x: x.replace('10.0.', ''))
    
    os_list = [v.split('.') for v in os.values]
    
    if is_debug:
        df_os = pd.DataFrame(os_list, columns=["OSVersion-Major", "OSVersion-Sub"])
    else:
        df_os = pd.DataFrame(os_list, columns=["OSVersion-Major", "OSVersion-Sub", '0', '1'])
    
    col = 'OSVersion-Major'
    feature = df_os[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'OSVersion-Sub'
    feature = df_os[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    del nlp_train[os_col]
    
    #========================================================================
    # Appのversionを分ける
    #========================================================================
    
    app_col = 'AppVersion'
    app = nlp_train[app_col]
    app = app.map(lambda x: x[2:])
    
    app_list = [v.split('.') for v in app.values]
    df_app = pd.DataFrame(app_list, columns=["AppVersion-1", "AppVersion-2", "AppVersion-3"])
    
    col = 'AppVersion-1'
    feature = df_app[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'AppVersion-2'
    feature = df_app[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    col = 'AppVersion-3'
    feature = df_app[col].values.astype('float32')
    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')
    nlp_train[col] = feature
    
    del nlp_train[app_col]
    print("Feature Engineering Complete!!")
    #========================================================================


#========================================================================
# Cleansing

new_cols = [col  if col in ignore_list else str(num) for num, col in enumerate(nlp_train.columns)]
nlp_train.columns = new_cols

nlp_dtype_dict = nlp_train.dtypes

for col in nlp_train.columns:
    if col in ignore_list:
        continue
        
    mode = nlp_train[col].mode().values[0]

    nlp_train[col].fillna(mode, inplace=True)
    
    if str(nlp_dtype_dict[col]).count('int') or str(nlp_dtype_dict[col]).count('float'):
        nlp_train[col] = nlp_train[col].map(lambda x: col + '_' + str(int(x)))
        
nlp_train.head()
#========================================================================


#========================================================================
# Tokenize
start_time = time.time()
print("Transforming...")

if is_make:
    ## Tokenize the sentences
    use_cols = [col for col in nlp_train.columns if col not in ignore_list]
    tx_col = "text"
    # nlp_train[tx_col] = nlp_train[use_cols].apply(lambda x: ' '.join([ str(tx) for tx in x]), axis=1)
    nlp_train[tx_col] = nlp_train[use_cols].apply(lambda x: ' '.join(x.values.tolist()), axis=1)
    utils.to_df_pkl(df=nlp_train[[key, tx_col, target]], path='../input/', fname=f'0305_MS_NLP_feat{len(use_cols)}')
else:
    base = utils.read_df_pkl(path='../input/base_Av*')
    #  len_train = base[~base[target].isnull()]
    nlp_train = utils.read_df_pkl(path='../input/0305_MS_NLP_feat*')
    nlp_train[target] = base[target].values

text_list = nlp_train[tx_col].values.tolist()

max_features = 10000
nb_words = max_features
max_length = 100
tokenizer = Tokenizer(num_words=max_features, split=" ")
tokenizer.fit_on_texts(text_list)
del text_list
gc.collect()

#========================================================================

#========================================================================
# Make Feature Matric
nlp_test = nlp_train[nlp_train[target].isnull()]
nlp_train = nlp_train[~nlp_train[target].isnull()]
y = nlp_train[target].values
tx_train = nlp_train[tx_col]
tx_test = nlp_test[tx_col]
del nlp_train, nlp_test
gc.collect()

start_time = time.time()
x_train = tokenizer.texts_to_sequences(tx_train)
x_test = tokenizer.texts_to_sequences(tx_test)
print(f"Tokenize Complete!!: {time.time() - start_time}")

# Pad the sentences 
start_time = time.time()
print("Pad Sequences Start!!")
train_word_sequences = pad_sequences(x_train, maxlen=max_length, padding='post')
test_word_sequences = pad_sequences(x_test, maxlen=max_length, padding='post')
# train_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')
# test_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')

pred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)
print(f"Pad Sequences Complete!!: {time.time() - start_time}")

#========================================================================
# Numericの結合
is_num = 1
if is_num:
    train_word_sequences = np.hstack((train_word_sequences, num_train.values))
    test_word_sequences = np.hstack((test_word_sequences, num_test.values))

print(f"Train: {train_word_sequences.shape} | Test: {test_word_sequences.shape}")
print(train_word_sequences[:1])
print(test_word_sequences[:1])
#========================================================================

#========================================================================
# Make Validation
seed = 1208
fold_n = 5
base = utils.read_df_pkl('../input/base_group*')
vi = utils.read_pkl_gzip('../input/f000_AvSigVersion.gz')
vi_col = 'f000_AvSigVersion'
base[vi_col] = vi
base_train = base[~base[target].isnull()]
base_test = base[base[target].isnull()]

base_train.sort_values(vi_col, inplace=True)

if is_debug:
    base_train = base_train[[key, target]].head(10000)
    base_test = base_test[[key, target]].head(1000)
else:
    base_train = base_train[[key, target]]
    base_test = base_test[[key, target]]

from sklearn.model_selection import KFold
# define the validation scheme
cv = KFold(n_splits=fold_n, shuffle=False, random_state=seed)
kfold = list(cv.split(base_train, base_train[target]))
# kfold = MS_utils.get_kfold(base=base)
# kfold = zip(*kfold)
#========================================================================

# hyperparameters
embedding_size = 200
learning_rate = 0.001
batch_size = 2**9
if is_debug: batch_size = 2**4
if is_multi: batch_size *= gpu_count
num_epoch = 10
max_length = train_word_sequences.shape[1]
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, verbose=0),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4)
]

base = base[[key, target]]
#========================================================================

tx_train = train_word_sequences
x_test = test_word_sequences
is_oof = 1

#========================================================================
# Result Box
model_list = []
result_list = []
score_list = []
val_pred_list = []
oof_pred = np.zeros(len(tx_train))
test_pred = np.zeros(len(x_test))
#========================================================================


#========================================================================
# Train & Prediction Start
for fold_no, (trn_idx, val_idx) in enumerate(kfold):

    #========================================================================
    # Make Dataset
    X_train, y_train = tx_train[trn_idx, :], y[trn_idx]
    X_val, y_val = tx_train[val_idx, :], y[val_idx]

    print(X_train.shape, X_val.shape)
    print(f"Target Min --- Train: {y_train.min()} Valid: {y_val.min()}")
    print(f"Target Min Count --- Train: {np.sum(y_train==y_train.min())} Valid: {np.sum(y_val==y_val.min())}")

    start_time = time.time()
    print("Start training ...")

    model = build_model(max_length=max_length, nb_words=nb_words, embedding_size=embedding_size)
    model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epoch-1, verbose=2, callbacks=callbacks)
    test_pred += 0.3*np.squeeze(model.predict(x_test, batch_size=batch_size, verbose=2))

    model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=2, verbose=2)
    test_pred += 0.7*np.squeeze(model.predict(x_test, batch_size=batch_size, verbose=2))

    print(f"Test Prob Shape: {test_pred.shape}")

    #========================================================================
    # OOF
    y_pred = np.squeeze(model.predict(X_val))
    oof_pred[val_idx] = y_pred
    #========================================================================

    del model
    gc.collect()
    K.clear_session()
    print("--- %s seconds ---" % (time.time() - start_time))

    # Scoring
    score = roc_auc_score(y_val, y_pred)
    print(f'AUC: {score}')
    score_list.append(score)
    #========================================================================

cv_score = np.mean(score_list)
logger.info(f'''
#========================================================================
# CV SCORE AVG: {cv_score}
#========================================================================''')

test_pred /= fold_no+1

base_train['prediction'] = oof_pred
base_test['prediction'] = test_pred

#========================================================================
# Stacking
if is_oof:
    df_stack = pd.concat([base_train, test], axis=0, ignore_index=True)
    print(f"DF Stack Shape: {df_stack.shape}")    
else:
    test_pred /= fold
    test['prediction'] = test_pred
    stack_test = test[[key, 'prediction']]
    result_list.append(stack_test)
    df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)
    df_stack = base.merge(df_pred, how='inner', on=key)
    print(f"Stacking Shape: {df_stack.shape}")
    del df_pred
    gc.collect()
#========================================================================

if is_debug:
    sys.exit()

utils.to_pkl_gzip(obj=df_stack, path=f'../stack/{start_time}_NN_NLP_feat{X_train.shape[1]}_fold{fold_n}_CV{cv_score}_LB')
