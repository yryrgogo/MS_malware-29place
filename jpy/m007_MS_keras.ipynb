{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (10000, 6) | Test: (500, 6)\n"
     ]
    }
   ],
   "source": [
    "is_debug = 1\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "HOME = os.path.expanduser('~')\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "sys.path.append(f\"../py/\")\n",
    "import MS_utils\n",
    "import utils, ml_utils, kaggle_utils\n",
    "from utils import logger_func\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "# Columns\n",
    "key, target, ignore_list = MS_utils.get_basic_var()\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "#========================================================================\n",
    "# Keras \n",
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "sys.path.append(f'{HOME}/kaggle/data_analysis/model')\n",
    "from nn_keras import MS_NN\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "#========================================================================\n",
    "\n",
    "start_time = \"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_group*')[[key, target, 'country_group']]\n",
    "\n",
    "# feat_path_list = glob.glob('../features/4_winner/*.gz')\n",
    "train, test = MS_utils.get_dataset(base=base, feat_path='../features/4_winner/*.gz', is_debug=False)\n",
    "del base\n",
    "gc.collect()\n",
    "\n",
    "if is_debug:\n",
    "    train = train.head(10000)\n",
    "    test = test.head(500)\n",
    "    \n",
    "#========================================================================\n",
    "# Categorical Encode\n",
    "cat_cols = utils.get_categorical_features(df=train, ignore_list=ignore_list)\n",
    "print(f\"Categorical: {cat_cols}\")\n",
    "\n",
    "#Fit LabelEncoder\n",
    "for col in cat_cols:\n",
    "    # 最も頻度の多いカテゴリでimpute\n",
    "    max_freq = list(train[col].value_counts().index)[0]\n",
    "    train[col].fillna(max_freq, inplace=True)\n",
    "    test[col].fillna(max_freq, inplace=True)\n",
    "    le = LabelEncoder().fit(pd.concat([train[col], test[col]], axis=0).value_counts().index.tolist())\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col]  = le.transform(test[col])\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化の前処理(Null埋め, inf, -infの処理) \n",
    "for col in train.columns:\n",
    "    if col in ignore_list: continue\n",
    "        \n",
    "    train[col] = utils.impute_feature(train, col)\n",
    "    test[col]  = utils.impute_feature(test, col)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# 正規化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_test = pd.concat([train, test], axis=0)\n",
    "base_train = train_test[~train_test[target].isnull()]\n",
    "base_test = train_test[train_test[target].isnull()]\n",
    "\n",
    "use_cols = [col for col in train.columns if col not in ignore_list]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_test[use_cols])\n",
    "    \n",
    "train = train_test[~train_test[target].isnull()]\n",
    "test = train_test[train_test[target].isnull()]\n",
    "\n",
    "x_test = scaler.transform(test[use_cols])\n",
    "         \n",
    "Y = train[target]\n",
    "\n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\") \n",
    "# ========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# CVの準備\n",
    "seed = 1208\n",
    "fold_n = 5\n",
    "kfold = MS_utils.get_kfold(base=base_train)\n",
    "kfold = zip(*kfold)\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(7997, 4) (2003, 4)\n",
      "Target Min --- Train: 0.0 Valid: 0.0\n",
      "Target Min Count --- Train: 3943 Valid: 1007\n",
      "Train on 7997 samples, validate on 2003 samples\n",
      "Epoch 1/2\n",
      " - 2s - loss: 0.9794 - acc: 0.5114 - val_loss: 0.9000 - val_acc: 0.5282\n",
      "Epoch 2/2\n",
      " - 0s - loss: 0.7761 - acc: 0.5269 - val_loss: 0.7330 - val_acc: 0.5467\n",
      "AUC: 0.5549810961821465\n",
      "(7998, 4) (2002, 4)\n",
      "Target Min --- Train: 0.0 Valid: 0.0\n",
      "Target Min Count --- Train: 3945 Valid: 1005\n",
      "Train on 7998 samples, validate on 2002 samples\n",
      "Epoch 1/2\n",
      " - 0s - loss: 0.7508 - acc: 0.5408 - val_loss: 0.7646 - val_acc: 0.5375\n",
      "Epoch 2/2\n",
      " - 0s - loss: 0.7503 - acc: 0.5128 - val_loss: 0.7136 - val_acc: 0.5529\n",
      "AUC: 0.5612025130116719\n",
      "(8000, 4) (2000, 4)\n",
      "Target Min --- Train: 0.0 Valid: 0.0\n",
      "Target Min Count --- Train: 3967 Valid: 983\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      " - 0s - loss: 0.7345 - acc: 0.5289 - val_loss: 0.7095 - val_acc: 0.5655\n",
      "Epoch 2/2\n",
      " - 0s - loss: 0.7364 - acc: 0.5229 - val_loss: 0.6905 - val_acc: 0.5615\n",
      "AUC: 0.5684577842996625\n",
      "(8002, 4) (1998, 4)\n",
      "Target Min --- Train: 0.0 Valid: 0.0\n",
      "Target Min Count --- Train: 3971 Valid: 979\n",
      "Train on 8002 samples, validate on 1998 samples\n",
      "Epoch 1/2\n",
      " - 0s - loss: 0.7378 - acc: 0.5234 - val_loss: 0.6840 - val_acc: 0.5656\n",
      "Epoch 2/2\n",
      " - 0s - loss: 0.7337 - acc: 0.5274 - val_loss: 0.6961 - val_acc: 0.5671\n",
      "AUC: 0.5913646838766199\n",
      "(8003, 4) (1997, 4)\n",
      "Target Min --- Train: 0.0 Valid: 0.0\n",
      "Target Min Count --- Train: 3974 Valid: 976\n",
      "Train on 8003 samples, validate on 1997 samples\n",
      "Epoch 1/2\n",
      " - 0s - loss: 0.7313 - acc: 0.5248 - val_loss: 0.6836 - val_acc: 0.5663\n",
      "Epoch 2/2\n",
      " - 0s - loss: 0.7321 - acc: 0.5331 - val_loss: 0.7008 - val_acc: 0.5663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-12 10:12:37,731 utils 90 [INFO]    [<module>] \n",
      "#========================================================================\n",
      "# CV SCORE AVG: 0.5705715732075183\n",
      "#======================================================================== \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.576851788667491\n",
      "DF Stack Shape: (10500, 7)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nn_keras import MS_NN\n",
    "#========================================================================\n",
    "is_oof = 1\n",
    "# NN Model Setting \n",
    "if is_debug:\n",
    "    N_EPOCHS = 2\n",
    "else:\n",
    "    N_EPOCHS = 10\n",
    "# learning_rate = 1e-4\n",
    "learning_rate = 1e-3\n",
    "first_batch=10 # 7: 128\n",
    "from adabound import AdaBound\n",
    "\n",
    "adabound = AdaBound(lr=learning_rate,\n",
    "                final_lr=0.1,\n",
    "                gamma=1e-03,\n",
    "                weight_decay=0.,\n",
    "                amsbound=False)\n",
    "\n",
    "\n",
    "model = MS_NN(input_cols=len(use_cols))\n",
    "metric = \"accuracy\"\n",
    "\n",
    "opt = optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=adabound, metrics=[metric])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4)\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Result Box\n",
    "model_list = []\n",
    "result_list = []\n",
    "score_list = []\n",
    "val_pred_list = []\n",
    "\n",
    "oof_pred = np.zeros(len(train))\n",
    "test_pred = np.zeros(len(test))\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(kfold):\n",
    "\n",
    "    #========================================================================\n",
    "    # Make Dataset\n",
    "    X_train, y_train = train.iloc[trn_idx, :][use_cols], Y.iloc[trn_idx]\n",
    "    X_val, y_val = train.iloc[val_idx, :][use_cols], Y.iloc[val_idx]\n",
    "    print(X_train.shape, X_val.shape)\n",
    "    print(f\"Target Min --- Train: {y_train.min()} Valid: {y_val.min()}\")\n",
    "    print(f\"Target Min Count --- Train: {np.sum(y_train==y_train.min())} Valid: {np.sum(y_val==y_val.min())}\")\n",
    "    \n",
    "    X_train[:] = scaler.transform(X_train)\n",
    "    X_val[:] = scaler.transform(X_val)\n",
    "    X_train = X_train.as_matrix()\n",
    "    X_val = X_val.as_matrix()\n",
    "    #========================================================================\n",
    "    \n",
    "    batch_size = 2**(first_batch)\n",
    "    model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val)\n",
    "              , batch_size=2**first_batch, epochs=N_EPOCHS\n",
    "              , verbose=2, callbacks=callbacks)\n",
    "    \n",
    "\n",
    "    #========================================================================\n",
    "    # OOF\n",
    "    y_pred = np.squeeze(model.predict(X_val))\n",
    "    oof_pred[val_idx] = y_pred\n",
    "    # Test\n",
    "    test_pred += np.squeeze(model.predict(x_test))\n",
    "    #========================================================================\n",
    "    \n",
    "    #========================================================================\n",
    "    # Scorring\n",
    "    score = roc_auc_score(y_val, y_pred)\n",
    "    print(f'AUC: {score}')\n",
    "    score_list.append(score)\n",
    "    #========================================================================\n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "test_pred /= fold_no+1\n",
    "\n",
    "base_train['prediction'] = oof_pred\n",
    "base_test['prediction'] = test_pred\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "df_stack = pd.concat([base_train, test], axis=0, ignore_index=True)\n",
    "print(f\"DF Stack Shape: {df_stack.shape}\")    \n",
    "utils.to_pkl_gzip(obj=df_stack[[key, 'prediction']], path=f'../stack/{start_time[4:12]}_MS_stack_NN_E{set_no+1}_batch{batch_size}_epoch{N_EPOCHS}_CV{cv_score}')\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
