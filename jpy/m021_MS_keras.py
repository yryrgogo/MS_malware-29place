is_debug = 1
import os
import re
import sys
import glob
import datetime
import pandas as pd
import numpy as np
HOME = os.path.expanduser('~')
sys.path.append(f"{HOME}/kaggle/data_analysis/library/")
sys.path.append(f"../py/")
import MS_utils
import utils, ml_utils, kaggle_utils
from utils import logger_func
try:
    if not logger:
        logger=logger_func()
except NameError:
    logger=logger_func()
import time
from sklearn.metrics import roc_auc_score, mean_squared_error
from sklearn.metrics import mean_squared_error, roc_auc_score
#========================================================================
# Keras 
# Corporación Favorita Grocery Sales Forecasting
sys.path.append(f'{HOME}/kaggle/data_analysis/model')
from nn_keras import MS_NN
from keras import callbacks
from keras import optimizers
from keras import backend as K
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
#========================================================================

start_time = "{0:%Y%m%d_%H%M%S}".format(datetime.datetime.now())

# Columns
key, target, ignore_list = MS_utils.get_basic_var()
comment = sys.argv[1]

base = utils.read_df_pkl('../input/base_group*')[[key, target, 'country_group']]

# feat_path_list = glob.glob('../features/4_winner/*.gz')
train, test = MS_utils.get_dataset(base=base, feat_path='../features/4_winner/*.gz', is_debug=False)
del base
gc.collect()

if is_debug:
    train = train.head(10000)
    test = test.head(500)
    
#========================================================================
# Categorical Encode
cat_cols = utils.get_categorical_features(df=train, ignore_list=ignore_list)
print(f"Categorical: {cat_cols}")

#Fit LabelEncoder
for col in cat_cols:
    # 最も頻度の多いカテゴリでimpute
    max_freq = list(train[col].value_counts().index)[0]
    train[col].fillna(max_freq, inplace=True)
    test[col].fillna(max_freq, inplace=True)
    le = LabelEncoder().fit(pd.concat([train[col], test[col]], axis=0).value_counts().index.tolist())
    train[col] = le.transform(train[col])
    test[col]  = le.transform(test[col])
#========================================================================

#========================================================================
# 正規化の前処理(Null埋め, inf, -infの処理) 
for col in train.columns:
    if col in ignore_list: continue
        
    train[col] = utils.impute_feature(train, col)
    test[col]  = utils.impute_feature(test, col)
#========================================================================

#========================================================================
# 正規化
from sklearn.preprocessing import StandardScaler

train_test = pd.concat([train, test], axis=0)
use_cols = [col for col in train.columns if col not in ignore_list]
scaler = StandardScaler()
scaler.fit(train_test[use_cols])
    
train = train_test[~train_test[target].isnull()]
test = train_test[train_test[target].isnull()]

base = train_test[[key, target]]
base_train = base[~base[target].isnull()]
base_test = base[base[target].isnull()]

del train_test
gc.collect()

x_test = scaler.transform(test[use_cols])
         
Y = train[target]

print(f"Train: {train.shape} | Test: {test.shape}") 
# ========================================================================

#========================================================================
# CVの準備
seed = 1208
fold_n = 5
kfold = MS_utils.get_kfold(base=base_train)
kfold = zip(*kfold)
#========================================================================


#========================================================================
# NN Setting
from nn_keras import MS_NN
# NN Model Setting 
if is_debug:
    N_EPOCHS = 2
else:
    N_EPOCHS = 10
# learning_rate = 1e-4
learning_rate = 1e-3
first_batch=10 # 7: 128
from adabound import AdaBound

adabound = AdaBound(lr=learning_rate,
                final_lr=0.1,
                gamma=1e-03,
                weight_decay=0.,
                amsbound=False)


model = MS_NN(input_cols=len(use_cols))
metric = "accuracy"

opt = optimizers.Adam(lr=learning_rate)
model.compile(loss="binary_crossentropy", optimizer=adabound, metrics=[metric])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=2, verbose=0),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4)
]
#========================================================================

#========================================================================
# Result Box
model_list = []
result_list = []
score_list = []
val_pred_list = []

oof_pred = np.zeros(len(train))
y_test = np.zeros(len(test))
#========================================================================

#========================================================================
# Train & Prediction Start

for num_fold, (trn_idx, val_idx) in enumerate(kfold):
    
    #========================================================================
    # 複数スレッドでkfoldを取得する
    get_fold = sys.argv[2].split('_')
    get_fold_list = [np.int(get_fold[0]), np.int(get_fold[1])]
    if num_fold not in get_fold_list:
        continue
    #========================================================================

    #========================================================================
    # Make Dataset
    x_train, y_train = train.iloc[trn_idx, :][use_cols], Y.iloc[trn_idx]
    x_val, y_val = train.iloc[val_idx, :][use_cols], Y.iloc[val_idx]
    print(x_train.shape, x_val.shape)
    
    x_train[:] = scaler.transform(x_train)
    x_val[:] = scaler.transform(x_val)
    x_train = x_train.as_matrix()
    x_val = x_val.as_matrix()
    #========================================================================
    
    batch_size = 2**(first_batch)
    model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val)
              , batch_size=2**first_batch, epochs=N_EPOCHS
              , verbose=2, callbacks=callbacks)

    #========================================================================
    # OOF
    y_pred = np.squeeze(model.predict(x_val))
    oof_pred[val_idx] = y_pred
    # Test
    y_test += np.squeeze(model.predict(x_test))
    #========================================================================
    
    #========================================================================
    # Scorring
    score = roc_auc_score(y_val, y_pred)
    print(f'AUC: {score}')
    score_list.append(score)
    #========================================================================
    
pred_col = 'prediction'
base[pred_col] = np.hstack((oof_pred, y_test))
base = base[[key, pred_col]]
print(f"DF Stack Shape: {base.shape}")    
#========================================================================
# Saving
utils.to_pkl_gzip(obj=base, path=f'../output/{start_time[4:12]}_{comment}_stack_{model_type}_FOLD-{sys.argv[2]}_feat{len(x_train.shape[1])}')
#========================================================================