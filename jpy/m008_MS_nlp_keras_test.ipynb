{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Train and Test Data.\n",
      "\n",
      "(16774736, 5)\n",
      "(8921483, 91) (7853253, 91)\n",
      "(8921483, 91) (7853253, 91)\n",
      "Data Load Complete!!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "is_make = 1\n",
    "is_debug = 0\n",
    "is_fe = 0\n",
    "is_multi = 0\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "HOME = os.path.expanduser('~')\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "sys.path.append(f\"../py/\")\n",
    "import MS_utils\n",
    "import utils, ml_utils, kaggle_utils\n",
    "from utils import logger_func\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "\n",
    "# Columns\n",
    "key, target, ignore_list = MS_utils.get_basic_var()\n",
    "\n",
    "\n",
    "if is_multi:\n",
    "    import tensorflow as tf\n",
    "    from keras.utils.training_utils import multi_gpu_model # add\n",
    "    gpu_count = 4 # add\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# Keras\n",
    "from os.path import dirname\n",
    "#sys.path.append(dirname(dirname(__file__)))\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "# https://github.com/bfelbo/DeepMoji/blob/master/deepmoji/attlayer.py\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "def build_model(nb_words, max_length, embedding_size=200):\n",
    "    #  with tf.device(\"/cpu:0\"): # add\n",
    "    inp = Input(shape=(max_length,))\n",
    "#     x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Embedding(nb_words, embedding_size, input_length=max_length)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2])\n",
    "    predictions = Dense(1, activation='sigmoid')(conc)\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    adam = optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "    if is_multi:\n",
    "        model = multi_gpu_model(model, gpus=gpu_count)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "#========================================================================\n",
    "\n",
    "base = utils.read_df_pkl(path='../input/base_Av*')\n",
    "if is_make:\n",
    "    #========================================================================\n",
    "    # Dataset Load\n",
    "    with utils.timer('Download Train and Test Data.\\n'):\n",
    "        train, test = MS_utils.get_dataset(base=base, feat_path='../features/4_winner/*.gz', is_cat_encode=False)\n",
    "\n",
    "\n",
    "        nlp_cols = [\n",
    "            'Engine'\n",
    "            ,'OSVersion'\n",
    "            ,'AppVersion'\n",
    "            ,'AvSigVersion'\n",
    "            ,'SkuEdition'\n",
    "            ,'SmartScreen'\n",
    "            ,'Census_OSArchitecture'\n",
    "            ,'AVProductStatesIdentifier'\n",
    "            ,'AVProductsInstalled'\n",
    "            ,'CountryIdentifier'\n",
    "            ,'CityIdentifier'\n",
    "            ,'OrganizationIdentifier'\n",
    "            ,'GeoNameIdentifier'\n",
    "            ,'LocaleEnglishNameIdentifier'\n",
    "            ,'OsBuild'\n",
    "            ,'OsBuildLab'\n",
    "            ,'Census_OEMNameIdentifier'\n",
    "            ,'Census_OEMModelIdentifier'\n",
    "            ,'Census_ProcessorCoreCount'\n",
    "            ,'Census_ProcessorManufacturerIdentifier'\n",
    "            ,'Census_PrimaryDiskTypeName'\n",
    "            ,'Census_OSBranch'\n",
    "            ,'Census_OSBuildNumber'\n",
    "            ,'Census_OSBuildRevision'\n",
    "            ,'Census_OSEdition'\n",
    "            ,'Census_OSInstallTypeName'\n",
    "            ,'Census_OSInstallLanguageIdentifier'\n",
    "            ,'Census_OSUILocaleIdentifier'\n",
    "            ,'Census_OSWUAutoUpdateOptionsName'\n",
    "            ,'Census_GenuineStateName'\n",
    "            ,'Census_ActivationChannel'\n",
    "            ,'Census_FlightRing'\n",
    "            ,'Census_FirmwareManufacturerIdentifier'\n",
    "            ,'Census_FirmwareVersionIdentifier'\n",
    "        ]\n",
    "\n",
    "        num_list = [\n",
    "            'Census_PrimaryDiskTotalCapacity'\n",
    "            ,'Census_SystemVolumeTotalCapacity'\n",
    "            ,'Census_TotalPhysicalRAM'\n",
    "            ,'Census_InternalPrimaryDiagonalDisplaySizeInInches'\n",
    "            ,'Census_InternalPrimaryDisplayResolutionHorizontal'\n",
    "            ,'Census_InternalPrimaryDisplayResolutionVertical'\n",
    "            ,'Census_InternalBatteryNumberOfCharges'\n",
    "            ,'Wdft_IsGamer'\n",
    "        ]\n",
    "\n",
    "        tmp_list = []\n",
    "        for col in train.columns:\n",
    "            for n_col in nlp_cols:\n",
    "                if col.count(n_col):\n",
    "                    tmp_list.append(col)\n",
    "        nlp_cols = list(set(tmp_list))\n",
    "\n",
    "        tmp_list = []\n",
    "        for col in train.columns:\n",
    "            for n_col in num_list:\n",
    "                if col.count(n_col):\n",
    "                    tmp_list.append(col)\n",
    "        num_list = list(set(tmp_list))\n",
    "\n",
    "        # Text\n",
    "        nlp_train = train[[key, target] + nlp_cols]\n",
    "        nlp_test = test[[key] + nlp_cols]\n",
    "\n",
    "        # Numeric\n",
    "        num_train = train[num_list]\n",
    "        num_test = test[num_list]\n",
    "\n",
    "        if is_debug:\n",
    "            nlp_train = nlp_train.head(10000)\n",
    "            nlp_test = nlp_test.head(1000)\n",
    "            num_train = num_train.head(10000)\n",
    "            num_test = num_test.head(1000)\n",
    "\n",
    "        nlp_train = pd.concat([nlp_train, nlp_test], axis=0)\n",
    "        nlp_train.drop(\"f000_AvSigVersion\", axis=1, inplace=True)\n",
    "        print(\"Data Load Complete!!\")\n",
    "        \n",
    "        del train, test\n",
    "        gc.collect()\n",
    "\n",
    "#========================================================================\n",
    "\n",
    "\n",
    "#========================================================================\n",
    "# FE\n",
    "\n",
    "\n",
    "if is_fe:\n",
    "    #========================================================================\n",
    "    # Engineのversionを分ける\n",
    "    #========================================================================\n",
    "\n",
    "    en_col = 'EngineVersion'\n",
    "    engine = nlp_train[en_col]\n",
    "    engine = engine.map(lambda x: x[4:])\n",
    "    df_en = pd.DataFrame([en.split('.') for en in engine.values], columns=['Engine-Major', 'Engine-Sub'])\n",
    "    col = 'Engine-Major'\n",
    "    feature = df_en[col].values.astype('float32')\n",
    "    nlp_train[col] = feature\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "\n",
    "    col = 'Engine-Sub'\n",
    "    feature = df_en[col].values.astype('float32')\n",
    "    nlp_train[col] = feature\n",
    "\n",
    "    del nlp_train[en_col]\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "\n",
    "    #========================================================================\n",
    "    # 'AvSigVersion'のversionを分ける\n",
    "    #========================================================================\n",
    "\n",
    "    vi_col = 'AvSigVersion'\n",
    "    vi = nlp_train[vi_col]\n",
    "    vi_pre = vi.map(lambda x: np.int(x[:4].replace('.', '').replace('&', '')))\n",
    "    vi_suf = vi.map(lambda x: np.int(x[5:].replace('.', '').replace(\"x17;311440\", '311440')))\n",
    "    \n",
    "    df_vi = pd.concat([vi_pre, vi_suf], axis=1)\n",
    "    df_vi.columns = ['AvSigVersion-Major', 'AvSigVersion-Sub']\n",
    "    \n",
    "    col = 'AvSigVersion-Major'\n",
    "    feature = df_vi[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    col = 'AvSigVersion-Sub'\n",
    "    feature = df_vi[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    \n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    del nlp_train[vi_col]\n",
    "    \n",
    "    #========================================================================\n",
    "    # OSのversionを分ける\n",
    "    #========================================================================\n",
    "    \n",
    "    os_col = 'Census_OSVersion'\n",
    "    os = nlp_train[os_col]\n",
    "    os = os.map(lambda x: x.replace('10.0.', ''))\n",
    "    \n",
    "    os_list = [v.split('.') for v in os.values]\n",
    "    \n",
    "    if is_debug:\n",
    "        df_os = pd.DataFrame(os_list, columns=[\"OSVersion-Major\", \"OSVersion-Sub\"])\n",
    "    else:\n",
    "        df_os = pd.DataFrame(os_list, columns=[\"OSVersion-Major\", \"OSVersion-Sub\", '0', '1'])\n",
    "    \n",
    "    col = 'OSVersion-Major'\n",
    "    feature = df_os[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    col = 'OSVersion-Sub'\n",
    "    feature = df_os[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    del nlp_train[os_col]\n",
    "    \n",
    "    #========================================================================\n",
    "    # Appのversionを分ける\n",
    "    #========================================================================\n",
    "    \n",
    "    app_col = 'AppVersion'\n",
    "    app = nlp_train[app_col]\n",
    "    app = app.map(lambda x: x[2:])\n",
    "    \n",
    "    app_list = [v.split('.') for v in app.values]\n",
    "    df_app = pd.DataFrame(app_list, columns=[\"AppVersion-1\", \"AppVersion-2\", \"AppVersion-3\"])\n",
    "    \n",
    "    col = 'AppVersion-1'\n",
    "    feature = df_app[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    col = 'AppVersion-2'\n",
    "    feature = df_app[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    col = 'AppVersion-3'\n",
    "    feature = df_app[col].values.astype('float32')\n",
    "    # utils.to_pkl_gzip(obj=feature, path=f'../features/1_first_valid/{prefix}_{col}')\n",
    "    nlp_train[col] = feature\n",
    "    \n",
    "    del nlp_train[app_col]\n",
    "    print(\"Feature Engineering Complete!!\")\n",
    "    #========================================================================\n",
    "    \n",
    "    \n",
    "#========================================================================\n",
    "# Cleansing\n",
    "with utils.timer(\"Transforming...\"):\n",
    "    if is_make:\n",
    "\n",
    "        org_cols = [col  if col in ignore_list else str(num) for num, col in enumerate(nlp_train.columns)]\n",
    "        new_cols = [col  if col in ignore_list else str(num) for num, col in enumerate(nlp_train.columns)]\n",
    "        nlp_train.columns = new_cols\n",
    "\n",
    "        nlp_dtype_dict = nlp_train.dtypes\n",
    "\n",
    "        for col in tqdm(nlp_train.columns):\n",
    "            if col in ignore_list:\n",
    "                continue\n",
    "\n",
    "            mode = nlp_train[col].mode().values[0]\n",
    "\n",
    "            nlp_train[col].fillna(mode, inplace=True)\n",
    "\n",
    "            if str(nlp_dtype_dict[col]).count('int') or str(nlp_dtype_dict[col]).count('float'):\n",
    "                nlp_train[col] = nlp_train[col].map(lambda x: col + '_' + str(int(x)))\n",
    "\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Tokenize\n",
    "with utils.timer(\"Transforming...\"):\n",
    "    if is_make:\n",
    "        ## Tokenize the sentences\n",
    "        use_cols = [col for col in nlp_train.columns if col not in ignore_list]\n",
    "        tx_col = \"text\"\n",
    "        # nlp_train[tx_col] = nlp_train[use_cols].apply(lambda x: ' '.join([ str(tx) for tx in x]), axis=1)\n",
    "        nlp_train[tx_col] = nlp_train[use_cols].apply(lambda x: ' '.join(x.values.tolist()), axis=1)\n",
    "        utils.to_df_pkl(df=nlp_train[[key, tx_col, target]], path='../input/', fname=f'0305_MS_NLP_feat{len(use_cols)}')\n",
    "    else:\n",
    "        #  len_train = base[~base[target].isnull()]\n",
    "        nlp_train = utils.read_df_pkl(path='../input/0305_MS_NLP_feat*')\n",
    "\n",
    "text_list = nlp_train[tx_col].values.tolist()\n",
    "\n",
    "print(nlp_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize Complete!!: 752.0716238021851\n",
      "Pad Sequences Start!!\n",
      "Pad Sequences Complete!!: 259.49582505226135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (8921483, 108) | Test: (7853253, 108)\n",
      "[[1.00000e+00 4.30000e+01 3.00000e+00 1.00000e+00 1.00000e+01 3.00000e+00\n",
      "  4.00000e+00 6.40000e+01 5.00000e+00 1.10000e+01 7.00000e+00 6.00000e+00\n",
      "  6.00000e+00 1.00000e+00 1.80000e+01 1.10000e+01 1.60000e+01 6.30000e+01\n",
      "  2.00000e+01 2.00000e+00 2.40000e+01 1.20000e+01 1.20000e+01 4.50000e+01\n",
      "  3.20000e+01 1.10000e+01 2.60000e+01 1.40000e+01 3.50000e+01 1.30000e+01\n",
      "  1.90000e+01 7.00000e+00 8.00000e+00 2.30000e+01 1.30000e+01 5.00000e+00\n",
      "  2.30000e+01 2.00000e+00 1.50000e+01 9.00000e+00 1.40000e+01 5.00000e+00\n",
      "  3.40000e+01 1.58100e+03 3.80000e+01 1.12000e+02 3.70000e+01 1.18000e+02\n",
      "  2.90000e+01 8.50000e+01 2.20000e+01 1.60000e+01 4.10000e+01 7.00000e+00\n",
      "  3.30000e+01 4.60000e+01 2.80000e+01 7.00000e+00 9.00000e+00 1.50000e+01\n",
      "  3.00000e+01 8.00000e+00 2.70000e+01 4.90000e+01 3.90000e+01 5.00000e+01\n",
      "  3.60000e+01 4.20000e+01 4.00000e+01 1.00200e+03 2.50000e+01 5.60000e+01\n",
      "  1.70000e+01 2.00000e+00 2.10000e+01 4.40000e+01 3.10000e+01 9.60000e+01\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 3.14700e+03 6.04900e+03\n",
      "  1.00000e+00 1.68700e+03 2.99600e+04 2.31013e+05 1.43000e+02 4.50000e+02]]\n",
      "[[1.0000e+00 4.3000e+01 3.0000e+00 1.0000e+00 1.0000e+01 2.0000e+00\n",
      "  4.0000e+00 1.8900e+02 5.0000e+00 1.1000e+01 7.0000e+00 6.0000e+00\n",
      "  6.0000e+00 1.0000e+00 1.8000e+01 1.1000e+01 1.6000e+01 4.1800e+02\n",
      "  2.0000e+01 2.0000e+00 2.4000e+01 7.0000e+00 1.2000e+01 5.9000e+01\n",
      "  3.2000e+01 1.1000e+01 2.6000e+01 4.0000e+00 3.5000e+01 9.0000e+00\n",
      "  1.9000e+01 5.0000e+00 8.0000e+00 9.0000e+01 1.3000e+01 2.0000e+00\n",
      "  2.3000e+01 1.0000e+00 1.5000e+01 9.0000e+00 1.4000e+01 5.0000e+00\n",
      "  3.4000e+01 1.1710e+03 3.8000e+01 1.0200e+02 3.7000e+01 1.0100e+02\n",
      "  2.9000e+01 9.4000e+01 2.2000e+01 1.6000e+01 4.1000e+01 1.0000e+01\n",
      "  3.3000e+01 8.7000e+01 2.8000e+01 4.0000e+00 9.0000e+00 1.5000e+01\n",
      "  3.0000e+01 8.0000e+00 2.7000e+01 6.6000e+01 3.9000e+01 4.0000e+00\n",
      "  3.6000e+01 8.2000e+01 4.0000e+01 1.7090e+03 2.5000e+01 5.7000e+01\n",
      "  1.7000e+01 4.0000e+00 2.1000e+01 7.4000e+01 3.1000e+01 3.0000e+02\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 4.9470e+03 6.1710e+03\n",
      "  1.0000e+00 8.2000e+01 5.0951e+04 0.0000e+00 1.0800e+02 9.3000e+02]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  6.21it/s]\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "nb_words = max_features\n",
    "max_length = 100\n",
    "tokenizer = Tokenizer(num_words=max_features, split=\" \")\n",
    "tokenizer.fit_on_texts(text_list)\n",
    "del text_list\n",
    "gc.collect()\n",
    "\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Make Feature Matric\n",
    "nlp_test = nlp_train[nlp_train[target].isnull()]\n",
    "nlp_train = nlp_train[~nlp_train[target].isnull()]\n",
    "y = nlp_train[target].values\n",
    "tx_train = nlp_train[tx_col].copy()\n",
    "tx_test = nlp_test[tx_col].copy()\n",
    "del nlp_train, nlp_test\n",
    "gc.collect()\n",
    "\n",
    "with utils.timer('Text to Sequences...'):\n",
    "    x_train = tokenizer.texts_to_sequences(tx_train)\n",
    "    x_test = tokenizer.texts_to_sequences(tx_test)\n",
    "    \n",
    "del tx_train, tx_test\n",
    "gc.collect(\n",
    "\n",
    "# Pad the sentences \n",
    "with utils.timer('Pad Sequences...'):\n",
    "    print(\"Pad Sequences Start!!\")\n",
    "    train_word_sequences = pad_sequences(x_train, maxlen=max_length, padding='post')\n",
    "    test_word_sequences = pad_sequences(x_test, maxlen=max_length, padding='post')\n",
    "    # train_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')\n",
    "    # test_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "    pred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)\n",
    "del x_train, x_test\n",
    "gc.collect(\n",
    "    \n",
    "#========================================================================\n",
    "# Numericの結合\n",
    "is_num = 1\n",
    "if is_num:\n",
    "    train_word_sequences = np.hstack((train_word_sequences, num_train.values))\n",
    "    test_word_sequences = np.hstack((test_word_sequences, num_test.values))\n",
    "\n",
    "print(f\"Train: {train_word_sequences.shape} | Test: {test_word_sequences.shape}\")\n",
    "print(train_word_sequences[:1])\n",
    "print(test_word_sequences[:1])\n",
    "\n",
    "# Train Test Set\n",
    "tx_train = train_word_sequences.copy()\n",
    "x_test = test_word_sequences.copy()\n",
    "# utils.to_df_pkl(df=pd.DataFrame(tx_train), path='../input', fname='0305_MS_NLP_train_only_feat')\n",
    "# utils.to_df_pkl(df=pd.DataFrame(x_test), path='../input', fname='0305_MS_NLP_test_only_feat')\n",
    "del train_word_sequences, test_word_sequences\n",
    "gc.collect()\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Make Validation\n",
    "start_time = time.time()\n",
    "seed = 1208\n",
    "fold_n = 5\n",
    "base = utils.read_df_pkl('../input/base_group*')\n",
    "vi = utils.read_pkl_gzip('../input/f000_AvSigVersion.gz')\n",
    "vi_col = 'f000_AvSigVersion'\n",
    "base[vi_col] = vi\n",
    "base_train = base[~base[target].isnull()]\n",
    "base_test = base[base[target].isnull()]\n",
    "base_train.sort_values(vi_col, inplace=True)\n",
    "\n",
    "if is_debug:\n",
    "    base_train = base_train[[key, target]].head(10000)\n",
    "    base_test = base_test[[key, target]].head(1000)\n",
    "else:\n",
    "    base_train = base_train[[key, target]]\n",
    "    base_test = base_test[[key, target]]\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# define the validation scheme\n",
    "cv = KFold(n_splits=fold_n, shuffle=False, random_state=seed)\n",
    "kfold = list(cv.split(base_train, base_train[target]))\n",
    "# kfold = MS_utils.get_kfold(base=base)\n",
    "# kfold = zip(*kfold)\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# hyperparameters\n",
    "embedding_size = 200\n",
    "learning_rate = 0.001\n",
    "batch_size = 2**9\n",
    "if is_debug: batch_size = 2**4\n",
    "if is_multi: batch_size *= gpu_count\n",
    "num_epoch = 10\n",
    "max_length = train_word_sequences.shape[1]\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4)\n",
    "]\n",
    "#========================================================================\n",
    "\n",
    "#========================================================================\n",
    "# Dataset\n",
    "base = base[[key, target]]\n",
    "is_oof = 1\n",
    "\n",
    "#========================================================================\n",
    "# Result Box\n",
    "result_list = []\n",
    "score_list = []\n",
    "oof_pred = np.zeros(len(tx_train))\n",
    "test_pred = np.zeros(len(x_test))\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Train & Prediction Start\n",
    "for fold_no, (trn_idx, val_idx) in enumerate(kfold):\n",
    "\n",
    "    with utils.timer(f'Fold{fold_no} Train Start!!'):\n",
    "        #========================================================================\n",
    "        # Make Dataset\n",
    "        X_train, y_train = tx_train[trn_idx, :], y[trn_idx]\n",
    "        X_val, y_val = tx_train[val_idx, :], y[val_idx]\n",
    "\n",
    "        print(X_train.shape, X_val.shape)\n",
    "        print(f\"Target Min --- Train: {y_train.min()} Valid: {y_val.min()}\")\n",
    "        print(f\"Target Min Count --- Train: {np.sum(y_train==y_train.min())} Valid: {np.sum(y_val==y_val.min())}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\"Start training ...\")\n",
    "\n",
    "        model = build_model(max_length=max_length, nb_words=nb_words, embedding_size=embedding_size)\n",
    "        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epoch-1, verbose=2, callbacks=callbacks)\n",
    "        test_pred += 0.3*np.squeeze(model.predict(x_test, batch_size=batch_size, verbose=2))\n",
    "\n",
    "        model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=2, verbose=2)\n",
    "        test_pred += 0.7*np.squeeze(model.predict(x_test, batch_size=batch_size, verbose=2))\n",
    "\n",
    "        print(f\"Test Prob Shape: {test_pred.shape}\")\n",
    "\n",
    "        #========================================================================\n",
    "        # OOF\n",
    "        y_pred = np.squeeze(model.predict(X_val))\n",
    "        oof_pred[val_idx] = y_pred\n",
    "        #========================================================================\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        # Scoring\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        print(f'AUC: {score}')\n",
    "        score_list.append(score)\n",
    "        #========================================================================\n",
    "\n",
    "cv_score = np.mean(score_list)\n",
    "logger.info(f'''\n",
    "#========================================================================\n",
    "# CV SCORE AVG: {cv_score}\n",
    "#========================================================================''')\n",
    "\n",
    "test_pred /= fold_no+1\n",
    "\n",
    "base_train['prediction'] = oof_pred\n",
    "base_test['prediction'] = test_pred\n",
    "\n",
    "#========================================================================\n",
    "# Stacking\n",
    "if is_oof:\n",
    "    df_stack = pd.concat([base_train, test], axis=0, ignore_index=True)\n",
    "    print(f\"DF Stack Shape: {df_stack.shape}\")    \n",
    "else:\n",
    "    test_pred /= fold\n",
    "    test['prediction'] = test_pred\n",
    "    stack_test = test[[key, 'prediction']]\n",
    "    result_list.append(stack_test)\n",
    "    df_pred = pd.concat(result_list, axis=0, ignore_index=True).drop(target, axis=1)\n",
    "    df_stack = base.merge(df_pred, how='inner', on=key)\n",
    "    print(f\"Stacking Shape: {df_stack.shape}\")\n",
    "    del df_pred\n",
    "    gc.collect()\n",
    "#========================================================================\n",
    "\n",
    "if is_debug:\n",
    "    sys.exit()\n",
    "\n",
    "utils.to_pkl_gzip(obj=df_stack, path=f'../stack/{start_time}_NN_NLP_feat{X_train.shape[1]}_fold{fold_n}_CV{cv_score}_LB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
