{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "HOME = os.path.expanduser('~')\n",
    "sys.path.append(f\"{HOME}/kaggle/data_analysis/library/\")\n",
    "import utils\n",
    "from utils import logger_func\n",
    "try:\n",
    "    if not logger:\n",
    "        logger=logger_func()\n",
    "except NameError:\n",
    "    logger=logger_func()\n",
    "\n",
    "key = 'MachineIdentifier'\n",
    "target = 'HasDetections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b130d0c90f9052e4e79d9a42a33126d326f99e76"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../tool/ctrNet-tool/')\n",
    "import ctrNet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src import misc_utils\n",
    "import os\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import glob\n",
    "# feat_path_list = glob.glob('../features/4_winner/*.gz')\n",
    "# p_list = utils.parallel_load_data(path_list=feat_path_list)\n",
    "# df_feat = pd.concat(p_list, axis=1)\n",
    "\n",
    "base = utils.read_df_pkl('../input/base_gr*')[[key, target, 'country_group']]\n",
    "len_train = base[~base[target].isnull()].shape[0]\n",
    "del p_list, base\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f24dcd1a223ea1175800f722952980c4a8a4215c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 118/141 [40:44<10:47, 28.14s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#========================================================================\n",
    "# Loading Dataset\n",
    "#========================================================================\n",
    "def make_bucket(data,num=10):\n",
    "    data.sort()\n",
    "    bins=[]\n",
    "    for i in range(num):\n",
    "        bins.append(data[int(len(data)*(i+1)//num)-1])\n",
    "    return bins\n",
    "\n",
    "float_features = utils.get_numeric_features(df_feat, ignore_list=[key, target])\n",
    "\n",
    "for f in tqdm(float_features):\n",
    "#     data=list(train[f])+list(test[f])\n",
    "    mode = df_feat[~df_feat[f].isnull()][f].mode()[0]\n",
    "    df_feat[f].fillna(mode, inplace=True)\n",
    "    data = df_feat[f].tolist()\n",
    "    bins=make_bucket(data,num=50)\n",
    "    df_feat[f] = np.digitize(df_feat[f], bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_list = [key, target]\n",
    "cat_cols = utils.get_categorical_features(df_feat, ignore_list=ignore_list)\n",
    "\n",
    "for col in cat_cols:\n",
    "    df_feat[col].fillna(0, inplace=True)\n",
    "\n",
    "train = df_feat.iloc[:len_train, :]\n",
    "test = df_feat.iloc[len_train:, :]\n",
    "features = [col for col in train.columns if col not in ignore_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40b2ecd8536dcac62d426daeeb8c5f79f8f817c4"
   },
   "outputs": [],
   "source": [
    "hparam=tf.contrib.training.HParams(\n",
    "            model='xdeepfm',\n",
    "            norm=True,\n",
    "            batch_norm_decay=0.9,\n",
    "            hidden_size=[128,128],\n",
    "            cross_layer_sizes=[128,128,128],\n",
    "            k=8,\n",
    "            hash_ids=int(2e5),\n",
    "            batch_size=1024,\n",
    "            optimizer=\"adam\",\n",
    "            learning_rate=0.001,\n",
    "            num_display_steps=1000,\n",
    "            num_eval_steps=1000,\n",
    "            epoch=1,\n",
    "            metric='auc',\n",
    "            activation=['relu','relu','relu'],\n",
    "            cross_activation='identity',\n",
    "            init_method='uniform',\n",
    "            init_value=0.1,\n",
    "            feature_nums=len(features),\n",
    "            kfold=5)\n",
    "misc_utils.print_hparams(hparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "66e67bbf4fb72dfb853ca1cb05ce993f670bec35"
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c48b409a4e86c09ca395c3a4f75b2b1c372d124"
   },
   "outputs": [],
   "source": [
    "index=set(range(train.shape[0]))\n",
    "K_fold=[]\n",
    "for i in range(hparam.kfold):\n",
    "    if i == hparam.kfold-1:\n",
    "        tmp=index\n",
    "    else:\n",
    "        tmp=random.sample(index,int(1.0/hparam.kfold*train.shape[0]))\n",
    "    index=index-set(tmp)\n",
    "    print(\"Number:\",len(tmp))\n",
    "    K_fold.append(tmp)\n",
    "    \n",
    "y_pred = np.zeros(train.shape[0])\n",
    "for i in range(hparam.kfold):\n",
    "    print(\"Fold\",i)\n",
    "    dev_index=K_fold[i]\n",
    "    dev_index=random.sample(dev_index,int(0.1*len(dev_index)))\n",
    "    train_index=[]\n",
    "    for j in range(hparam.kfold):\n",
    "        if j!=i:\n",
    "            train_index+=K_fold[j]\n",
    "            \n",
    "    x_train = train.iloc[train_index][features]\n",
    "    y_train = train.iloc[train_index]['HasDetections']\n",
    "    x_val = train.iloc[dev_index][features]\n",
    "    y_val = train.iloc[dev_index]['HasDetections']\n",
    "    \n",
    "    model=ctrNet.build_model(hparam)\n",
    "    model.train(train_data=(x_train, y_train), dev_data=(x_val, y_val))\n",
    "    print(\"Training Done! Inference...\")\n",
    "    if i==0:\n",
    "        y_pred[dev_index] += model.infer(dev_data=(x_val, y_val))/hparam.kfold\n",
    "        y_test = model.infer(dev_data=(test[features], test['HasDetections']))/hparam.kfold\n",
    "    else:\n",
    "        y_pred[dev_index] += model.infer(dev_data=(x_val, y_val))/hparam.kfold\n",
    "        y_test += model.infer(dev_data=(test[features],test['HasDetections']))/hparam.kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4302884724f2191b363591d5af43c1dc12c79a48"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcd62d5f09ca9d78e96854d84433325fb80f6096"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission['HasDetections'] = preds\n",
    "print(submission['HasDetections'].head())\n",
    "submission.to_csv('nffm_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
